# 2025-08-11, Monday

## üéØ Goals for Today

- [x] Finish the [Linear combinations and span](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-combinations/v/linear-combinations-and-span) section
- [x] In PyTorch --> [[03-pytorch-linear-combination.ipynb]]
  - [x] Implement linear combinations of two 2D vectors with variable coefficients
  - [x] Plot with matplotlib

## üìñ What I Studied

- We can reach any vector in a space using a linear combination of $n$ vectors, provided those vectors are linearly independent and span the space

## üí° Insights and Reflections

- A **linear combination** of vectors $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_n$ is any vector of the form $c_1\vec{v}_1 + c_2\vec{v}_2 + \dots + c_n\vec{v}_n$, where each $c_i \in \mathbb{R}$.
- We can represent a vector in $\mathbb{R}^2$ with a linear combination of vectors $\vec a$ and $\vec b$
- **Linear combination**: $c_1\vec a + c_2\vec b$ is a vector formed by scaling $\vec a$ and $\vec b$ by scalars $c_1, c_2$ and adding them
- The **span** of a set of vectors is the set of all possible linear combinations of those vectors: $\text{span}(\vec{v}_1, \vec{v}_2, \dots, \vec{v}_n) = \left\{ c_1 \vec{v}_1 + c_2 \vec{v}_2 + \dots + c_n \vec{v}_n \ \middle|\ c_i \in \mathbb{R} \right\}$
- If the vectors are **collinear** (linearly dependent), their span is a line, not the whole space
- Example of dependence::
  - $\vec a = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$, $\vec{b} = \begin{bmatrix} -2 \\ -2 \end{bmatrix}$ --> $\vec a + \vec b = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
- $span(\vec a)$ is all the vectors lying on the line of scaled vector $\vec a$
- $span(\vec v_1, \vec v_2, ..., \vec v_n) = \{c_1v_1 + c_2v_2 + ... + c_nv_n\}, c \in \mathbb{R}^2$ for $1 \leq i \leq n$
- We can call $c_1, c_2, ..., c_n$ weights

![Linear combination example 1](assets/Screenshot%20From%202025-08-11%2008-37-58.png)

![Linear combination example 2](assets/Screenshot%20From%202025-08-11%2008-39-27.png)

### Example

$\vec a = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$, $\vec{b} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}$, $\vec{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$

$c_1\vec a + c_2\vec b = \vec x$

$c_1\begin{bmatrix} 1 \\ 2 \end{bmatrix} + c_2\begin{bmatrix} 0 \\ 3 \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$

$1c_1 + 0c_2 = x_1$ -> $c_1 = x_1$
$2c_1 + 3c_2 = x_2$ -> $c_2 = \frac{1}{3} (x_2 - 2x_1)$

Now, let's prove that we can represent a vector $\vec x = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$ with scaled vectors $\vec a$ and $\vec b$
$x_1 = 2$, $x_2 = 2$

$c_1 = 2$, $c_2 = \frac{1}{3}(2 - 2*2) = \frac{1}{3}(-2) = -\frac{2}{3}$

$2\begin{bmatrix} 1 \\ 2 \end{bmatrix} + -\frac{2}{3}\begin{bmatrix} 0 \\ 3 \end{bmatrix} = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$

$\begin{bmatrix} 2 \\ 4 \end{bmatrix} + \begin{bmatrix} 0 \\ -2 \end{bmatrix} = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$

## ‚ùì Questions

- What is linear dependence and independence?
