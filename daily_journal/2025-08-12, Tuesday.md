# 2025-08-12, Tuesday

## ğŸ¯ Goals for Today

- [x] Finish Linear dependence and independence section (Khan Academy)
  - [x] Linear dependence
  - [x] Linear independence
  - [x] Geometric interpretation
- [x] In PyTorch: check dependence/independence using `torch.linalg.matrix_rank`
- [x] Make Anki cards

> Make up 2â€“3 of your own examples (both dependent and independent) and test them.

## ğŸ“– What I Studied

### Linear Independence

**Linearly dependent vectors**

Colinear vectors:

![Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹](assets/Screenshot%20From%202025-08-13%2010-38-25.png)

We call set of vectors $\large\{\begin{bmatrix} 2 \\ 3 \end{bmatrix}, \begin{bmatrix} 4 \\ 6 \end{bmatrix}\large\}$ is called **linearly dependent**. One of the vector in the set can be represented by some combination of other vectors in the set. Whichever vector you pick which can be represented by the others is not adding any new directionality, any new information.

**Another example**

Anything in $\mathbb{R}^2$ can be represented with $\vec v_1$, $\vec v_2$, and we see that in our linear dependent set $\vec v_3$ is redundant, i.e. it can be represented as a linear combination of $\vec v_1$ and $\vec v_2$.

![ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ 1](assets/Pasted%20image%2020250813105324.png)

**Linearly independent vectors**

Let's take two vectors $\vec v_1 = \begin{bmatrix} 7 \\ 0 \end{bmatrix}$ and $\vec v_2 = \begin{bmatrix} 0 \\ -1 \end{bmatrix}$. Can we represent any of this vectors with another one? Obviously, not. Therefore, such a pair of vectors is called **linearly independent**.

Another example of linear independent vectors:

![ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹](assets/Pasted%20image%2020250813113609.png)

### More on Linear Independence

**Linear Dependence**

$s = \large\{ \vec v_1, \vec v_2, ... , \vec v_n\large\}$ *linearly dependent* `iff` (*if and only if*) $c_1\vec v_1 + c_2\vec v_2 + ... + c_n\vec v_n = \mathbb{0} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$, i.e. for some $c_i$ not all vectors are zero => at least 1 is not zero.

$\vec v_1 = a_2\vec v_2 + a_3\vec v_3 + ... a_n\vec v_n$
$0 = -1\vec v_1 + a_2\vec v_2 + a_3\vec v_3 + ... + a_n\vec v_n$, i.e. at least one vector ($-1\vec v_1$) is non zero.

---

$\large\{\begin{bmatrix} 2 \\ 1 \end{bmatrix}, \begin{bmatrix} 3 \\ 2 \end{bmatrix}\large\}$ â€” ?

$c_1\begin{bmatrix} 2 \\ 1 \end{bmatrix} + c_2\begin{bmatrix} 3 \\ 2 \end{bmatrix} = 0 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

$c_1$ or $c_2$ nonzero => dependent
$c_1$ and $c_2$ zero => independent

$2c_1 + 3c_2 = 0$
$c_1 + 2c_2 = 0$

...
$c_1 = 0, c_2 = 0$

We've just proved that both vectors are **linearly independent** â€” you can't represent one as a combination of the other.

---

$\large\{\begin{bmatrix} 2 \\ 1 \end{bmatrix}, \begin{bmatrix} 3 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 2 \end{bmatrix}\large\}$ â€” ?

$c_1\begin{bmatrix} 2 \\ 1 \end{bmatrix} + c_2\begin{bmatrix} 3 \\ 2 \end{bmatrix} + c_3\begin{bmatrix} 1 \\ 2 \end{bmatrix} = 0 = \begin{bmatrix} 0 \\ 0 \\ 0\end{bmatrix}$

$2c_1 + 3c_2 + 1c_3 = 0$
$1c_1 + 2c_2 + 2c_3 = 0$

> If you have 3 2-dimensional vectors, one of them will be *redundant*, because in a very best case when vectors $\vec v_1$ and $\vec v_2$ are *linearly independent* $span(\vec v_1, \vec v_2) = \mathbb{R}^2$ -> we don't need the third vector for that.

If $c_3 = -1$:

$2c_1 + 3c_2 - 1 = 0$
$1c_1 + 2c_2 -2 = 0$ -> let's multiply by 2

$2c_1 + 3c_2 - 1 = 0$
$2c_1 + 4c_2 - 4 = 0$

$-c_2 = -3$ -> $c_2 = 3$
$c_1 + 6 - 2 = 0$ -> $c_1 = 4$

So, $-1\begin{bmatrix} 2 \\ 1 \end{bmatrix} + 3\begin{bmatrix} 3 \\ 2 \end{bmatrix} + 4\begin{bmatrix} 1 \\ 2 \end{bmatrix} = 0 = \begin{bmatrix} 0 \\ 0 \\ 0\end{bmatrix}$ -> *linearly dependent vectors*

### Span and linear independence example

$\{\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}, \begin{bmatrix} 2 \\ 1 \\ 3 \end{bmatrix}, \begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix}\} = S$

1. is $span(S) = \mathbb{R}^3$ ?
2. are these vectors *linearly independent*?

To span $\mathbb{R}^3$, some linear combination of the vectors should construct any vector in this space.

$c_1\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix} + c_2\begin{bmatrix} 2 \\ 1 \\ 3 \end{bmatrix} + c_3\begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix} = \begin{bmatrix} a \\ b \\ c \end{bmatrix}$

$\begin{bmatrix} 1c_1 \\ -1c_1 \\ 2c_1 \end{bmatrix} + \begin{bmatrix} 2c_2 \\ 1c_2 \\ 3c_2 \end{bmatrix} + \begin{bmatrix} -1c_3 \\ 0c_3 \\ 2c_3 \end{bmatrix}$ ->

$1c_1 + 2c_2 - 1c_3 = a$
$-1c_1 + 1c_2 + 0c_3 = b$
$2c_1 + 3c_2 + 2c_3 = c$

Let's sum the first two equations up -> $3c_2 - c_3 = b + a$
Now let's multiple the first equation by -2 and add it to the third ->
$-2c_1 - 4c_2 + 2c_3 = -2a$ -> $-c_2 + 4c_3 = c - 2a$

$c_1 + 2c_2 - c_3 = a$
$3c_2 - c_3 = b + a$
$-c_2 + 4c_3 = c - 2a$ -> let's eliminate $c_2$ by multiplying it by $3$ -> $-3c_2 - 12c_3 = -3c + 6a$
and now sum the second and third equations up -> $11c_3 = 3c - 6a + b + a$ -> $11c_3 = 3c - 5a + b$

=> $c_3 = \frac{1}{11}(3c - 5a + b)$
=> $3c_2 = b + a + c_3$ => $c_2 = \frac{1}{3}(b + a + c_3)$
=> $c_1 = a - 2c_2 + c_3$

Now we can say that a set of these 3 vectors does indeed span $\mathbb{R}^3$.

$c_1\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix} + c_2\begin{bmatrix} 2 \\ 1 \\ 3 \end{bmatrix} + c_3\begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$

If these vectors are **linearly dependent**, there must be some *non-zero solution*. At least one of the constants $c_1$, $c_2$, $c_3$ will be non-zero for this solution.

If these vectors are **linearly independent**, all the constants should be *zero*, i.e.

$\begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} a \\ b \\ c \end{bmatrix}$ => $a = b = c = 0$

=> $c_3 = \frac{1}{11}(3*0 - 5*0* + 0)$
=> $c_2 = \frac{1}{3}(0 + 0 + 0)$
=> $c_1 = 0 - 2*0 + 0$

No vectors are redundant here.

## ğŸ’» PyTorch Practice

**Task:** Write a small function to check if a set of vectors is linearly independent using matrix rank.

```python
import torch

def is_independent(vectors):
    mat = torch.stack(vectors)  # stack vectors into a matrix
    rank = torch.linalg.matrix_rank(mat)
    return rank == len(vectors)

# Examples
v1 = torch.tensor([1., 2., 3.])
v2 = torch.tensor([4., 5., 6.])
v3 = torch.tensor([7., 8., 9.])  # Dependent on v1 and v2

print("v1, v2 independent?", is_independent([v1, v2]))
print("v1, v2, v3 independent?", is_independent([v1, v2, v3]))
```

## ğŸ’¡ Insights and Reflections

- **Linear independence** means no vector in the set can be expressed as a linear combination of the others. Itâ€™s a way of checking whether each vector adds a â€œnewâ€ direction to the space.
- **Matrix rank** gives the dimension of the space spanned by the vectors (number of independent directions). If the rank equals the number of vectors, theyâ€™re independent; if not, some are redundant.
- In the example, $v_3$ is dependent because it lies in the plane formed by $v_1$ and $\vec v_2$ (in fact, itâ€™s $\vec v_1 + \vec v_2$$ shifted by a constant factor).
- **Why it matters**:
  - For a **basis**, all vectors must be independent; otherwise, they canâ€™t uniquely represent points in the space.
  - The **dimension** of a space is exactly the size of the largest set of independent vectors it contains.
- In deep learning, independence is conceptually related to avoiding redundant features or parameters â€” independent directions in the data/features carry unique information, improving model expressiveness and numerical stability.

## â“ Questions

- How to determine dependence (definition + computational method via rank)?
- Why independence matters for basis and dimension?
