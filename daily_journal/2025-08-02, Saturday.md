# 2025-08-02, Saturday

#ML #DL #week1

## 🎯 Goals for Today

- [x] Watch [Part 1: Getting started](https://course.fast.ai/Lessons/lesson1.html)
- [x] Set up the environment with `uv`

## 📖 What I Studied

- How the fast.ai's course is organized
- Difference between machine learning (ML) and deep learning (ML)

## 💡 Insights and Reflections

- In 2015, identifying a bird in a picture was nearly an impossible task. Now we can build such a system in just a few minutes — all thanks to deep learning and hardware advancements.
- For computer vision (CV) models, it’s easy to inspect the data just by looking at it.
- With all the advancements happening in AI, data ethics is crucial for building safe systems. Check out the [Practical Data Ethics](https://ethics.fast.ai) course from fast.ai.
- There’s also a book called _Deep Learning for Coders with fastai & PyTorch_, which complements the course. Try reading one chapter after each lecture.
- Image-based algorithms aren't limited to images — they can be applied to other types of data as well. Techniques like CNNs are used in other domains (e.g., NLP, audio, time series).
- The fastai library (a layered API for deep learning) is built on top of [[PyTorch]] and eliminates much of the boilerplate. → [documentation](https://docs.fast.ai)
- The Jupyter Notebooks for the fast.ai course are also available on [Kaggle](https://www.kaggle.com/jhoward/code).
- It’s a good idea to resize images to around 400px to reduce loading time and speed up training for neural networks.
- In deep learning, a `batch` refers to a small number of examples from the dataset processed together → `dls.show_batch(max_n=6)`
- timm is a deep learning library created by [Ross Wightman](https://twitter.com/wightmanr). It contains a collection of state-of-the-art (SOTA) computer vision models, layers, utilities, optimizers, data loaders, augmentations, and training/validation scripts. fastai integrates with it: <https://timm.fast.ai>
- We can download pre-trained model weights and **fine-tune** the model — that is, teach it the differences between our dataset and the original one it was trained on.
- The first neural network was created in 1957. The basic ideas haven’t changed much 🤯 — but now we have GPUs, SSDs, and vast amounts of data.

## 🤩 Inspirations

![[assets/Screenshot 2025-08-02 at 13.16.04.png|500]]

Radek Osmulski’s journey is incredibly inspiring — from learning to code at 29, through fast.ai, to landing a role at @NVIDIAAI on the Merlin team.

He also wrote a book:  

**_Meta Learning: How to Learn Deep Learning and Thrive in the Digital World_**,  

where he shares the mindset, tools, and tactics that helped him go from beginner to practitioner.

[📄 Meta Learning Notes (PDF)](assets/Meta%20Learning%20How%20to%20Learn%20Deep%20Learning%20and%20Thrive%20in%20the%20Digital%20World%20Notes.pdf)

## ❓ Questions

- What is a neural network *in terms of numbers*?
  - Is it just multiplying things together and adding them up?
  - How does it learn to replicate patterns like turning negatives into zeros?

- How does backpropagation actually work at a low level?
  - What are we computing and updating during training?

- Why are neural networks so effective for image tasks compared to traditional algorithms?

- What is a “batch” and why do we use batches instead of feeding one image at a time?

- What does “fine-tuning” mean, and how is it different from training a model from scratch?

- How do we know if our model will generalize well to new (unseen) data?

- What’s happening under the hood when I call `learn.fit_one_cycle()` in fastai?

- How does fastai integrate with PyTorch under the surface?
  - When should I use fastai vs raw PyTorch?

- What role does normalization play in image inputs?

- What is the actual meaning of “activations” and “weights” in a trained model?
