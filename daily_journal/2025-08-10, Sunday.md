# 2025-08-10, Sunday

## üéØ Goals for Today

- In #PyTorch:
  - [x] Create 1D tensors
  - [x] Perform addition & scalar multiplication
  - [x] Verify results manually with paper/mental math
- Anki practice ‚Äî 20 min, decks:
  - [x] What is ML & Neural Networks
  - [x] Essential Algebra: Vectors
- Review Khan Academy notes on vector addition & scalar multiplication

## üìñ What I Studied

- How to perform the basic operations with tensors in PyTorch

## üí° Insights and Reflections

### PyTorch Tensors

> [Introduction to PyTorch Tensors](https://docs.pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)

In PyTorch, a Tensor is the fundamental data structure ‚Äî basically a multi-dimensional array that can run on both CPU and GPU, similar to NumPy‚Äôs `ndarray` but with extra powers:

- **Stores numerical data**
  - Scalars: `tensor(3.14)` ‚Üí 0D
  - Vectors: `tensor([1, 2, 3])` ‚Üí 1D
  - Matrices: `tensor([[1, 2], [3, 4]])` ‚Üí 2D
  - Higher-dimensional arrays for images, videos, batches, etc.
- **Supports automatic differentiation**
  - If `requires_grad=True`, PyTorch tracks operations on the tensor so it can compute gradients automatically (used for training neural networks).
- **Optimized for hardware acceleration**
  - Can live in CPU or GPU memory
  - Operations can be vectorized and parallelized for speed
- **Interoperable with NumPy**
  - You can easily convert between `numpy.ndarray` and `torch.Tensor` without copying data (when on CPU)

If you think in math terms:

> **Tensor** ‚Äî generalization of scalars (0D), vectors (1D), and matrices (2D) to n-dimensions.

| Name               | Rank (Dimensions) | Example Code in PyTorch                              | Shape       |
| ------------------ | ----------------- | ---------------------------------------------------- | ----------- |
| Scalar             | 0                 | `torch.tensor(42)`                                   | `()`        |
| Vector             | 1                 | `torch.tensor([1, 2, 3])`                            | `(3,)`      |
| Matrix             | 2                 | `torch.tensor([[1, 2],[3, 4]])`                      | `(2, 2)`    |
| Higher-rank Tensor | 3+                | `torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])` | `(2, 2, 2)` |

> In tensors, **rank** means ‚Äúnumber of dimensions‚Äù (axes), not matrix rank from linear algebra. That‚Äôs a common confusion point.

In machine learning, tensors represent data (features, images, audio) and model parameters.

| Concept in Math         | In Linear Algebra                    | In PyTorch (or NumPy) |
| ----------------------- | ------------------------------------ | --------------------- |
| **Scalar**              | Single number                        | 0-D tensor            |
| **Vector**              | Ordered list of numbers (1D array)   | 1-D tensor            |
| **Matrix**              | Table of numbers with rows & columns | 2-D tensor            |
| **Higher-order tensor** | Multi-indexed array (e.g., 3D cube)  | 3-D+ tensor           |

Higher-rank tensors have 3+ axes, which don‚Äôt have a special name in classical linear algebra but appear naturally in physics, graphics, and ML (e.g., RGB image batches).

Operations you know for vectors (addition, scalar multiplication, dot product) are _special cases_ of tensor operations. They work on any dimension, often in a _vectorized_ way.

### Real-world Examples

| Name               | Rank | PyTorch example                                  | Shape          | Real‚Äëworld example                                  |
|--------------------|------|---------------------------------------------------|----------------|-----------------------------------------------------|
| Scalar             | 0    | `torch.tensor(21.5)`                              | `()`           | Temperature value (e.g., 21.5‚ÄØ¬∞C)                   |
| Vector             | 1    | `torch.randn(300)`                                | `(300,)`       | Word embedding (300‚ÄëD)                              |
| Matrix             | 2    | `torch.randn(480, 640)`                           | `(480, 640)`   | Grayscale image (height √ó width)                    |
| Rank‚Äë3 tensor      | 3    | `torch.randn(480, 640, 3)`                        | `(480, 640, 3)`| RGB image (height √ó width √ó channels)               |

### Vectorization

When we say **‚ÄúPyTorch vectorizes things‚Äù**, we mean that PyTorch applies the operation **to every element of the tensor at once**, without you writing an explicit `for` loop.

PyTorch does the looping _internally_ in optimized C/CUDA code, so:

- It‚Äôs much **faster** (can use CPU SIMD instructions or GPU parallelism)
- It‚Äôs **cleaner** to read and write
- The same syntax works for scalars, vectors, matrices, and higher-rank tensors

So when you write `a + b` in PyTorch, it‚Äôs doing **element-wise addition** across all elements at once ‚Äî that‚Äôs vectorization.

## ‚ùì Questions

How vectorization works internally in optimized C/CUDA code?

![Vectorization in PyTorch](assets/Screenshot%20From%202025-08-10%2022-04-22.png)
